{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week Four Task\n",
    "====\n",
    "\n",
    "Team Member: 顾炜，杨光\n",
    "\n",
    "*All the codes and files (week 1 to 4) are avalaible at [github repo](https://github.com/4everer/stormTasks)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务要求**\n",
    ">任务描述：\n",
    "\n",
    ">在任务3的基础上，实现新闻类网页的分类功能。训练数据和测试数据通过公众号下载。\n",
    "要求是，分类器的训练模型可以使用非Storm技术，新网页的分类必须使用Storm技术。\n",
    "\n",
    ">最终输出：\n",
    "\n",
    "> 1. 形成《使用Storm实现新闻分词处理报告》（包含算法说明，程序运算准确率（即分类正确的网页个数/总网页个数），程序流程图，测试说明，结果介绍）\n",
    "> 2. 使用Storm实现新闻分类程序源代码；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 程序说明\n",
    "\n",
    "## 程序结构\n",
    "(流程图)\n",
    "\n",
    "本周项目分为两部分： 分类模型训练部分和storm部分。\n",
    "\n",
    "首先介绍文本分类的方法和使用的模型，然后是具体的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于文本分类\n",
    "\n",
    "### 模型介绍\n",
    "#### Naive Bayesian\n",
    "\n",
    "#### SVM\n",
    "\n",
    "#### xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "分类模型训练使用python编写，有数据读入，模型训练和优化，模型存储三部分。主要使用了第三方机器学习库[sklearn](http://scikit-learn.org/)，还增加了xgboost分类算法。\n",
    "\n",
    "(flow chart of machine learning)\n",
    "\n",
    "**requirements**:\n",
    "- sklearn\n",
    "- xgboost\n",
    "- matplotlib\n",
    "- chardet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据读入\n",
    "可以使用sklearn提供的数据，得到的数据类型是`sklearn.datasets.base.Bunch`，是一个类似于字典的类型，其中文本存在`data`中，label存在`target`中，以数字标明，而分类的名称在`target_names`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n",
      "comp.sys.mac.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.graphics\n",
      "sci.space\n",
      "talk.politics.guns\n",
      "sci.med\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_data = fetch_20newsgroups(shuffle=True, random_state=42)\n",
    "\n",
    "all_categories = [twenty_data.target_names[t] for t in twenty_data.target]\n",
    "for s in all_categories[:10]:\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "target_names = twenty_data.target_names\n",
    "pprint(target_names)\n",
    "type(twenty_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，从提供的数据文件读入。\n",
    "\n",
    "其中需要注意encoding，这里使用的chardet库来判断file的编码，以下是读入的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import chardet\n",
    "\n",
    "def load_data(datadir):\n",
    "\n",
    "    for f in listdir(datadir):\n",
    "        if isfile(join(datadir, f)):\n",
    "            char_encoding = chardet.detect(f)[\"encoding\"]\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    print \"============================\"\n",
    "    print \"read files from \" + datadir\n",
    "    print \"file encoding is \" + str(char_encoding)\n",
    "    \n",
    "    dataset = load_files(datadir, encoding=char_encoding, decode_error=\"ignore\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据整理\n",
    "训练数据和测试数据的建立非常重要，可以避免over fitting。针对NB和SVM，采用0.3的比例得到train和test两组数据。（以下使用sklearn提供的数据结构，与提供的数据文件内容是相同的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "twenty_train, twenty_test, y_train, y_test = train_test_split(\n",
    "    twenty_data.data, twenty_data.target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而针对文本数据，首先需要对数据进行分词计数，向量化，之后计算tf-idf，这样的得到的数据就可以用于模型训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(twenty_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练\n",
    "使用整理好的数据，就可以进行模型的训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "export_clf_nb = MultinomialNB().fit(train_tfidf, y_train)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "export_clf_svm = (SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, n_iter=5, random_state=42)\n",
    "                            .fit(train_tfidf, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用下面这个简单的例子测试一下我们刚刚训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb predicts: 'God is love' => soc.religion.christian\n",
      "nb predicts: 'OpenGL on the GPU is fast' => rec.autos\n",
      "svm predicts: 'God is love' => soc.religion.christian\n",
      "svm predicts: 'OpenGL on the GPU is fast' => rec.autos\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "for model_name, clf in zip([\"nb\", \"svm\"], [export_clf_nb, export_clf_svm]):\n",
    "    predicted_new = clf.predict(X_new_tfidf)\n",
    "\n",
    "    for doc, category in zip(docs_new, predicted_new):\n",
    "        print( model_name + ' predicts: %r => %s' % (doc, target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上过程最简明清晰可以采用Pipeline的形式，NB和SVM的训练如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def pipeline_train_classifier(classifier, x_train, y_train):\n",
    "    if classifier == \"nb\":\n",
    "        text_clf_nb = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', MultinomialNB()),\n",
    "                            ])\n",
    "        return text_clf_nb.fit(x_train, y_train)\n",
    "    elif classifier == \"svm\":\n",
    "        text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                                 ('tfidf', TfidfTransformer()),\n",
    "                                 ('clf_svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                  alpha=1e-4, n_iter=5, random_state=42))\n",
    "                                ])\n",
    "        return text_clf_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 简单评估\n",
    "利用`sklearn.metrics.classsification_report`函数和之前的到的测试集，可以简单地评估模型的质量，包括准确率，召回率和F1。\n",
    "\n",
    "- 准确率\n",
    "- 召回率\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.85      0.75      0.80       139\n",
      "           comp.graphics       0.86      0.75      0.80       168\n",
      " comp.os.ms-windows.misc       0.72      0.89      0.80       148\n",
      "comp.sys.ibm.pc.hardware       0.70      0.81      0.75       170\n",
      "   comp.sys.mac.hardware       0.91      0.86      0.88       165\n",
      "          comp.windows.x       0.95      0.77      0.85       183\n",
      "            misc.forsale       0.93      0.59      0.73       185\n",
      "               rec.autos       0.84      0.92      0.88       178\n",
      "         rec.motorcycles       0.94      0.93      0.94       181\n",
      "      rec.sport.baseball       0.92      0.92      0.92       168\n",
      "        rec.sport.hockey       0.87      0.99      0.93       164\n",
      "               sci.crypt       0.83      0.98      0.90       197\n",
      "         sci.electronics       0.95      0.77      0.85       186\n",
      "                 sci.med       0.95      0.93      0.94       176\n",
      "               sci.space       0.92      0.95      0.93       192\n",
      "  soc.religion.christian       0.50      0.99      0.66       182\n",
      "      talk.politics.guns       0.81      0.96      0.88       170\n",
      "   talk.politics.mideast       0.92      0.98      0.95       162\n",
      "      talk.politics.misc       0.99      0.52      0.68       163\n",
      "      talk.religion.misc       1.00      0.11      0.20       118\n",
      "\n",
      "             avg / total       0.87      0.83      0.82      3395\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.89      0.94      0.91       139\n",
      "           comp.graphics       0.80      0.87      0.83       168\n",
      " comp.os.ms-windows.misc       0.86      0.90      0.88       148\n",
      "comp.sys.ibm.pc.hardware       0.83      0.79      0.81       170\n",
      "   comp.sys.mac.hardware       0.94      0.92      0.93       165\n",
      "          comp.windows.x       0.92      0.83      0.87       183\n",
      "            misc.forsale       0.83      0.92      0.87       185\n",
      "               rec.autos       0.90      0.94      0.92       178\n",
      "         rec.motorcycles       0.96      0.95      0.96       181\n",
      "      rec.sport.baseball       0.97      0.95      0.96       168\n",
      "        rec.sport.hockey       0.94      0.99      0.97       164\n",
      "               sci.crypt       0.99      0.97      0.98       197\n",
      "         sci.electronics       0.95      0.86      0.90       186\n",
      "                 sci.med       0.98      0.96      0.97       176\n",
      "               sci.space       0.96      0.99      0.97       192\n",
      "  soc.religion.christian       0.94      0.96      0.95       182\n",
      "      talk.politics.guns       0.94      0.97      0.95       170\n",
      "   talk.politics.mideast       0.95      0.99      0.97       162\n",
      "      talk.politics.misc       0.96      0.87      0.91       163\n",
      "      talk.religion.misc       0.92      0.77      0.84       118\n",
      "\n",
      "             avg / total       0.92      0.92      0.92      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classifier_metrics(clf, x_test, y_test, target_names=None):\n",
    "    predict = clf.predict(x_test)\n",
    "    return metrics.classification_report(y_test, predict, target_names=target_names)\n",
    "\n",
    "x_test = tfidf_transformer.transform(count_vect.transform(twenty_test))\n",
    "for clf in [export_clf_nb, export_clf_svm]:\n",
    "    print classifier_metrics(clf, x_test, y_test, target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型在经过优化评估之后就可以储存了（优化过程见下文），使用pickle来dump需要用到的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('export.pickle', 'wb') as f:\n",
    "    pickle.dump([count_vect, tfidf_transformer, target_names, [export_clf_nb, export_clf_svm]], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost\n",
    "本周项目还简单测试了xgboost模型。xgboost是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(train_tfidf, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'num_class':20,\n",
    "    'silent':0,\n",
    "    'objective':'multi:softprob',\n",
    "    'seed':42\n",
    "    }\n",
    "\n",
    "param['max_depth'] = 5\n",
    "param['nthread'] = 4\n",
    "param['learning_rate'] = 0.3\n",
    "param['n_estimators'] = 1000\n",
    "param['min_child_weight'] = 2\n",
    "param['subsample'] = 0.8\n",
    "param['eta'] = 1\n",
    "param['gamma'] = 0.5\n",
    "param['colsample_bytree'] = 0.8\n",
    "\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "num_round = 10\n",
    "plst = param.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.392931\ttrain-merror:0.346003\n",
      "[1]\teval-merror:0.335788\ttrain-merror:0.268089\n",
      "[2]\teval-merror:0.303976\ttrain-merror:0.231974\n",
      "[3]\teval-merror:0.289249\ttrain-merror:0.205834\n",
      "[4]\teval-merror:0.273638\ttrain-merror:0.185756\n",
      "[5]\teval-merror:0.264507\ttrain-merror:0.169971\n",
      "[6]\teval-merror:0.258910\ttrain-merror:0.155701\n",
      "[7]\teval-merror:0.253314\ttrain-merror:0.147115\n",
      "[8]\teval-merror:0.249485\ttrain-merror:0.133603\n",
      "[9]\teval-merror:0.245655\ttrain-merror:0.126657\n"
     ]
    }
   ],
   "source": [
    "num_round = 10\n",
    "plst = param.items()\n",
    "bst = xgb.train(plst, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类模型的建立\n",
    "### 关于文本分类\n",
    "\n",
    "### 模型介绍\n",
    "#### Naive Bayesian\n",
    "\n",
    "#### SVM\n",
    "\n",
    "#### xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storm topology\n",
    "(结构图)\n",
    "\n",
    "本次topology的功能是分析读取输入的一系列网址，逐个发送网址到读取url的bolt，然后获得网页内容，利用已经建立好的模型进行分类，最终汇总，输出结果。共有以下几个组件：\n",
    "\n",
    "- `SendUrlSpout`\n",
    "    在本项目中，读取还有网址的文件，然后逐行按条发送。\n",
    "- `ReadFromUrlBolt`\n",
    "    使用第三周的代码，接收网址的数据流，然后发出http请求，得到html源代码，解析后输出网页的标题和文字内容。\n",
    "- `VectorizePredictBolt`\n",
    "    读取已经训练好模型，对网页文字进行处理，然后利用模型预测分类。\n",
    "- `XgbPredict`\n",
    "    主要用于展示扩展性，可以在原有的模型之外，插入新的模型，进行预测。过程同样是读取训练好模型（不同的模型），对文字进行处理，最后预测分类，输出结果。\n",
    "- `Aggregator`\n",
    "    集中所有预测的分类，整合以待下一步处理。使用了`fieldsGrouping`按照`url`或是`title`作为key，将结果放在同一bolt汇总。 在本周的项目中，预测的结果在这一步的cleanup方法中输出到console和文件。\n",
    "\n",
    "# 结果介绍\n",
    "## 模型训练和优化\n",
    "前文简单介绍了模型的训练和测试结果，而在实际运用中，需要很多的优化，包括参数的选择，特征的构造和选择等等。在不断追求更好结果的同时，也需要注意防止过拟合的发生。\n",
    "\n",
    "### 关于数据集的优化\n",
    "任意打开数据集中的一篇文章，我们就会发现，其中包含了大量的元数据，比如这一篇\n",
    "\n",
    "```\n",
    "From: lerxst@wam.umd.edu (where's my thing)\n",
    "Subject: WHAT car is this!?\n",
    "Nntp-Posting-Host: rac3.wam.umd.edu\n",
    "Organization: University of Maryland, College Park\n",
    "Lines: 15\n",
    "\n",
    "I was wondering if anyone out there could enlighten me on this car I saw\n",
    "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
    "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
    "the front bumper was separate from the rest of the body. This is\n",
    "all I know. If anyone can tellme a model name, engine specs, years\n",
    "of production, where this car is made, history, or whatever info you\n",
    "have on this funky looking car, please e-mail.\n",
    "\n",
    "Thanks,\n",
    "- IL\n",
    "   ---- brought to you by your neighborhood Lerxst ----\n",
    "```\n",
    "\n",
    "除了正文之外，有发信人，组织，行数等事实上相关性较小的信息，删去之后会提高模型在预测新的数据，比如网页新闻的准确性。\n",
    "\n",
    "所以在读取数据时进行以下处理\n",
    "\n",
    "``` python\n",
    "twenty_data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "### 模型参数的优化\n",
    "事实上，要优化模型，应当将数据集分为三部分，**train**，**cross validation**和**test**。训练集用来训练模型，cross validation部分用来测试不同参数组合的预测效果，最终在测试数据集上检验模型的质量，这同样是为了避免过拟合。\n",
    "\n",
    "在本项目中，使用了sklearn和xgboost自带的cross validation。下面是SVM模型参数调试部分的过程，使用了`GridSearchCV`方法。\n",
    "\n",
    "#### grid search for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "## 参数范围的设置\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.001, 0.0001, 0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "## 数据读取，也使用了不含元数据的训练集\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "data_no_meta = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "def gridSearch(data, name):\n",
    "    print(\"Performing grid search for \" + name)\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "gridSearch(data, \"data with meta\")\n",
    "print(\"=============\")\n",
    "gridSearch(data_no_meta, \"data without meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的结果如下，这样就可以使用优化过的参数进行预测了\n",
    "\n",
    "```\n",
    "Performing grid search for data with meta\n",
    "pipeline: ['vect', 'tfidf', 'clf']\n",
    "parameters:\n",
    "{'clf__alpha': (0.001, 0.0001, 1e-05, 1e-06),\n",
    " 'clf__penalty': ('l2', 'elasticnet'),\n",
    " 'vect__max_df': (0.5, 0.75, 1.0),\n",
    " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
    "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
    "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.0min\n",
    "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 18.6min finished\n",
    "done in 1142.334s\n",
    "\n",
    "Best score: 0.922\n",
    "Best parameters set:\n",
    "    clf__alpha: 0.0001\n",
    "    clf__penalty: 'l2'\n",
    "    vect__max_df: 1.0\n",
    "    vect__ngram_range: (1, 2)\n",
    "\n",
    "=============\n",
    "\n",
    "Performing grid search for data without meta\n",
    "pipeline: ['vect', 'tfidf', 'clf']\n",
    "parameters:\n",
    "{'clf__alpha': (0.001, 0.0001, 1e-05, 1e-06),\n",
    " 'clf__penalty': ('l2', 'elasticnet'),\n",
    " 'vect__max_df': (0.5, 0.75, 1.0),\n",
    " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
    "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
    "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.8min\n",
    "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 13.0min finished\n",
    "done in 802.951s\n",
    "\n",
    "Best score: 0.752\n",
    "Best parameters set:\n",
    "    clf__alpha: 0.0001\n",
    "    clf__penalty: 'l2'\n",
    "    vect__max_df: 0.5\n",
    "    vect__ngram_range: (1, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost优化\n",
    "由于篇幅原因以及xgboost在测试中表现并不好，应该是优化不到位，模型没有建立好，所以优化过程就不介绍了。但原理上仍然是cross validation，固定大部分参数，然后测试找到一个参数的最优，再固定优化好的，测试剩余的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对实际网页的测试\n",
    "简单选取了一些网页如下\n",
    "\n",
    "**sample list**\n",
    "\n",
    "\n",
    "得到了下面的预测结果，依次是SVM， Naive Bayesian和Xgboost的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
